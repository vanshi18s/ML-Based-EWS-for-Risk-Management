# ML-Based Early Warning System for Risk Management

## ðŸ“‹ Project Overview

A comprehensive **Machine Learning-powered Early Warning System (EWS)** that classifies trading days into discrete risk levels (Low, Medium, High) by integrating:
- **Quantitative Market Data**: Price dynamics via OHLCV history
- **Qualitative News Sentiment**: Real-time analysis via Google Gemini API

This system eliminates look-ahead bias through time-series-safe feature engineering and provides an interactive Streamlit dashboard for risk monitoring.

---

## ðŸŽ¯ Problem Statement & Objectives

### Problem
Traditional risk metrics (Volatility, VaR) ignore qualitative market signals, while pure sentiment systems lack market-derived risk grounding. This creates a gap in holistic risk assessment.

### Objectives
1. âœ… Collect and preprocess historical stock data using `yfinance`
2. âœ… Engineer time-series-safe features using expanding-window statistics
3. âœ… Train a multinomial Logistic Regression pipeline for risk classification
4. âœ… Integrate Google Gemini API for structured news sentiment extraction
5. âœ… Deploy an interactive Streamlit dashboard for real-time monitoring

---

## ðŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA ACQUISITION LAYER                     â”‚
â”‚  â€¢ yfinance (OHLCV historical data)                          â”‚
â”‚  â€¢ CSV file upload (user-provided data)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FEATURE ENGINEERING LAYER                        â”‚
â”‚  â€¢ Daily Returns: pct_change()                               â”‚
â”‚  â€¢ Volatility (Annualized): expanding().std() Ã— âˆš252        â”‚
â”‚  â€¢ VaR 95%: expanding().quantile(0.05) Ã— âˆš252              â”‚
â”‚  â€¢ Sharpe Ratio: (return - rf) / volatility                 â”‚
â”‚  â€¢ Momentum (5-day): Close.pct_change(5)                    â”‚
â”‚  â€¢ High-Low Range: (High - Low) / Close                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RISK LABELING LAYER                              â”‚
â”‚  Scoring Mechanism:                                          â”‚
â”‚  â€¢ High Volatility (>75th %ile): +2 points                 â”‚
â”‚  â€¢ High VaR (<-2%): +2 points                               â”‚
â”‚  â€¢ Negative Sharpe Ratio: +1 point                          â”‚
â”‚  â€¢ Class Mapping:                                           â”‚
â”‚    - Low Risk: score < 2                                    â”‚
â”‚    - Medium Risk: 2 â‰¤ score < 4                            â”‚
â”‚    - High Risk: score â‰¥ 4                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                             â”‚
        â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ML MODEL LAYER  â”‚      â”‚  EXTERNAL SENTIMENT API â”‚
â”‚                  â”‚      â”‚  â€¢ Google Gemini API    â”‚
â”‚ â€¢ 80/20 Split    â”‚      â”‚  â€¢ News extraction      â”‚
â”‚   (Chronological)â”‚      â”‚  â€¢ JSON sentiment       â”‚
â”‚ â€¢ Scaling:       â”‚      â”‚  â€¢ Risk scoring (0-10)  â”‚
â”‚   StandardScaler â”‚      â”‚                         â”‚
â”‚ â€¢ Model:         â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   Logistic       â”‚
â”‚   Regression     â”‚
â”‚   (L2 Reg)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           STREAMLIT DASHBOARD & VISUALIZATION                â”‚
â”‚  â€¢ Real-time risk predictions                               â”‚
â”‚  â€¢ Interactive plots (Volatility, VaR, Sharpe)             â”‚
â”‚  â€¢ News sentiment display                                   â”‚
â”‚  â€¢ CSV download functionality                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Š Feature Engineering (Time-Series Safe)

### Expanding Window Approach
All features use `expanding()` windows to prevent **look-ahead bias**:

| Feature | Formula | Interpretation |
|---------|---------|-----------------|
| **Daily Return** | `(Close_t - Close_{t-1}) / Close_{t-1}` | Daily percentage change |
| **Volatility (Annualized)** | `expanding().std() Ã— âˆš252` | Risk magnitude (annualized) |
| **VaR 95%** | `expanding().quantile(0.05) Ã— âˆš252` | Worst-case 5% loss scenario |
| **Sharpe Ratio** | `(Î¼_return - rf) / Ïƒ` | Return per unit risk (rf=5% annual) |
| **Momentum (5-day)** | `Close.pct_change(5)` | 5-day price momentum |
| **High-Low Range** | `(High - Low) / Close` | Intra-day volatility proxy |

### Why Expanding Windows?
- âœ… **No data leakage**: Each day only uses past data
- âœ… **Realistic simulation**: Mimics live deployment
- âœ… **Adaptive metrics**: Features evolve as market history grows

---

## ðŸ† Risk Classification Logic

### Risk Scoring Mechanism

```python
def classify_risk(row):
    """
    Multi-criteria scoring based on distribution percentiles
    Avoids arbitrary thresholds, uses data-driven quantiles
    """
    score = 0
    
    # Volatility criterion
    if volatility > 75th percentile:
        score += 2
    elif volatility > 25th percentile:
        score += 1
    
    # VaR criterion
    if var < -2%:
        score += 2
    elif var < -1%:
        score += 1
    
    # Sharpe criterion
    if sharpe_ratio < 0:
        score += 1
    
    # Final classification
    return:
        0 if score < 2      â†’ "Low Risk"
        1 if 2 â‰¤ score < 4  â†’ "Medium Risk"
        2 if score â‰¥ 4      â†’ "High Risk"
```

---

## ðŸ¤– Machine Learning Pipeline

### Model Selection: Multinomial Logistic Regression

**Why Logistic Regression?**
1. âœ… **Interpretability**: Clear feature coefficients
2. âœ… **Probabilistic output**: Risk scores (0-1) for each class
3. âœ… **Handles multiclass**: 3-way classification (Low/Medium/High)
4. âœ… **Regularization**: L2 penalty prevents overfitting

### Training Configuration

```python
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# Data split: 80% train, 20% test (chronological)
split_idx = int(len(df) * 0.8)
X_train = X[:split_idx]
X_test = X[split_idx:]

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Model
model = LogisticRegression(
    solver="lbfgs",        # Handles multi-class well
    C=0.01,                # Strong L2 regularization
    penalty="l2",
    max_iter=2000,
    random_state=42,
    n_jobs=-1              # Parallel processing
)

model.fit(X_train_scaled, y_train)
```

---

## ðŸ“ˆ Experimental Results (Amazon Case Study)

### Dataset
- **Stock**: AMZN (Amazon Inc.)
- **Period**: 2010â€“2023 (13 years)
- **Observations**: 3,522 trading days
- **Split**: 80% train (2,818 days), 20% test (704 days)

### Performance Metrics

| Metric | Value | Interpretation |
|--------|-------|-----------------|
| **Accuracy** | 0.808 | 80.8% of predictions correct |
| **Weighted Precision** | 0.918 | 91.8% reliability across classes |
| **Weighted Recall** | 0.808 | 80.8% coverage of actual risk days |
| **F1-Score** | 0.835 | Balanced precision-recall performance |

### Confusion Matrix
```
                Predicted
              Low  Medium  High
Actual Low     X     Y      Z
       Medium  A     B      C
       High    D     E      F
```
- **High Precision for High-Risk**: Model effectively identifies dangerous days
- **Balanced Recall**: Captures most risk events without excessive false alarms

---

## ðŸŒ Google Gemini API Integration

### Workflow

```
1. INPUT: Company Name (e.g., "Apple Inc.")
           â†“
2. SEARCH QUERY: "Find latest news for Apple Inc." (Google Search tool)
           â†“
3. NEWS CONTEXT: 5-7 recent articles aggregated
           â†“
4. ANALYSIS PROMPT: "Analyze provided news for financial risks"
           â†“
5. JSON RESPONSE SCHEMA:
   {
     "risk_score": 6.5,          // 0-10 scale
     "sentiment": "Negative",     // Positive/Negative/Neutral
     "summary": "Recent supply chain disruptions..."
   }
           â†“
6. OUTPUT: Display in Streamlit dashboard
```

### API Configuration

```python
def get_gemini_sentiment(company_name, api_key):
    """
    Two-step process:
    1. Search for latest news using Google Search tool
    2. Analyze news with structured JSON output
    """
    api_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent"
    
    # Step 1: Search for news
    search_payload = {
        "contents": [{"parts": [{"text": search_query}]}],
        "tools": [{"google_search": {}}],
        "systemInstruction": {"parts": [{"text": search_system_prompt}]}
    }
    
    # Step 2: Analyze with JSON schema
    json_schema = {
        "type": "OBJECT",
        "properties": {
            "risk_score": {"type": "NUMBER"},
            "sentiment": {"type": "STRING"},
            "summary": {"type": "STRING"}
        },
        "required": ["risk_score", "sentiment", "summary"]
    }
    
    analyze_payload = {
        "contents": [{"parts": [{"text": analyze_query}]}],
        "generationConfig": {
            "responseMimeType": "application/json",
            "responseSchema": json_schema
        }
    }
```

---

## ðŸš€ Deployment

### Local Development

```bash
# Clone repository
git clone https://github.com/yourusername/ml-ews-risk-management.git
cd ml-ews-risk-management

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt

# Run Streamlit app
streamlit run ewsml.py
```

### Streamlit Cloud Deployment

1. **Push to GitHub**
   ```bash
   git add .
   git commit -m "Initial commit: ML EWS with Gemini integration"
   git push origin main
   ```

2. **Deploy on Streamlit Cloud**
   - Go to [share.streamlit.io](https://share.streamlit.io)
   - Connect GitHub repository
   - Select `ewsml.py` as main file
   - Add secrets (Gemini API key) in Settings

3. **Set Environment Variables** (Streamlit Cloud Secrets)
   ```toml
   # .streamlit/secrets.toml
   gemini_api_key = "your-api-key-here"
   ```

## ðŸ“ Project Structure

```
ml-ews-risk-management/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ ewsml.py                          # Main Streamlit application
â”œâ”€â”€ .gitignore                        # Git ignore rules
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml                   # Streamlit configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_amzn.csv               # Sample data for testing
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb     # EDA and data analysis
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb  # Feature generation
â”‚   â””â”€â”€ 03_model_training.ipynb       # ML pipeline
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download_data.py              # Download historical data
â””â”€â”€ docs/
    â”œâ”€â”€ METHODOLOGY.md                # Detailed methodology
    â”œâ”€â”€ API_SETUP.md                  # Gemini API configuration
    â””â”€â”€ RESULTS.md                    # Comprehensive results
```

---

## ðŸ“¦ Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| `streamlit` | 1.38.0 | Web dashboard framework |
| `pandas` | 2.2.2 | Data manipulation |
| `numpy` | 2.1.1 | Numerical computing |
| `plotly` | 5.24.0 | Interactive visualizations |
| `scikit-learn` | 1.5.1 | Machine learning models |
| `requests` | 2.32.3 | HTTP requests (Gemini API) |

**Install all dependencies:**
```bash
pip install -r requirements.txt
```

---

## ðŸ”‘ API Setup

### Google Gemini API

1. **Get API Key**
   - Visit [Google AI Studio](https://aistudio.google.com/app/apikey)
   - Create new API key
   - Copy and save securely

2. **Set in Streamlit Dashboard**
   - Enter API key in sidebar text input
   - Or set as environment variable:
     ```bash
     export GEMINI_API_KEY="your-key-here"
     ```

3. **Enable Required APIs**
   - Google Generative Language API
   - Google Search API (for news retrieval)

---

## ðŸ’¡ Usage Guide

### Step 1: Prepare Data
- Format: CSV with columns `Date, Open, High, Low, Close, Volume`
- Example:
  ```csv
  Date,Open,High,Low,Close,Volume
  2010-01-04,130.0,130.61,129.62,130.00,123432400
  2010-01-05,130.01,131.08,129.69,130.46,150476000
  ```

### Step 2: Upload to Dashboard
1. Open Streamlit app
2. Click "Upload CSV file" in sidebar
3. Select prepared CSV file

### Step 3: Configure Analysis
1. Enter company name (e.g., "Apple Inc.")
2. Enter stock symbol (e.g., "AAPL") - for context
3. Input Gemini API Key
4. Check "Enable news-based sentiment"

### Step 4: View Results
- **Data Cleaning**: Original vs cleaned shape and period
- **Risk Metrics**: Volatility, Sharpe, VaR visualizations
- **Risk Distribution**: Bar chart of Low/Medium/High cases
- **Model Performance**: Accuracy, Precision, Recall, F1-Score
- **Predictions**: Last 20 trading days with risk labels
- **News Analysis**: Latest sentiment and risk score from Gemini
- **Download**: Export predictions as CSV

---

## ðŸ”¬ Advanced Features

### Feature Importance
```python
# Get feature coefficients from trained model
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'coefficient': log_reg.coef_[0]
}).sort_values('coefficient', key=abs, ascending=False)
```

### Hyperparameter Tuning
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.001, 0.01, 0.1, 1],
    'solver': ['lbfgs', 'saga'],
    'max_iter': [1000, 2000, 3000]
}

grid_search = GridSearchCV(
    LogisticRegression(penalty='l2', random_state=42),
    param_grid,
    cv=5,
    scoring='f1_weighted'
)
```

### Multi-Stock Portfolio
```python
stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN']
results = {}

for stock in stocks:
    data = yf.download(stock, start='2010-01-01', end='2023-12-31')
    # Run pipeline
    results[stock] = model_predictions
```

---

## ðŸŽ“ Key Insights & Lessons

### 1. Look-Ahead Bias Prevention
- âŒ **Wrong**: Use rolling window fitted on entire dataset
- âœ… **Right**: Use expanding window (past data only)

### 2. Time-Series Train-Test Split
- âŒ **Wrong**: Random shuffle before split
- âœ… **Right**: Chronological split (80% historical, 20% recent)

### 3. Feature Scaling for Classification
- âœ… Always scale features before Logistic Regression
- âœ… Fit scaler on training data, apply to test

### 4. Class Imbalance Handling
- Use `average='weighted'` in metrics for imbalanced classes
- Consider `class_weight='balanced'` in model

### 5. External Data Integration
- âœ… Sentiment as **complementary signal**, not replacement
- âœ… Validate API responses thoroughly
- âœ… Handle timeouts and rate limits gracefully

---

## ðŸ“š References

### Core Libraries
- [yfinance Documentation](https://yfinance.readthedocs.io/)
- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [Plotly Python Documentation](https://plotly.com/python/)

### Academic Resources
- Jorion, P. (2006). *Value at Risk: The New Benchmark for Managing Financial Risk*
- Sharpe, W. F. (1994). "The Sharpe Ratio"
- Markowitz, H. (1952). *Portfolio Selection*

### API References
- [Google Generative AI API](https://ai.google.dev/tutorials/python_quickstart)
- [Google Search API](https://developers.google.com/custom-search)

---

## ðŸ‘¤ Author

**Vanshi Sethi**
- Roll Number: 23035010020
- Program: B.Sc. (Honours) Data Science and AI
- Institution: IIT Guwahati
- Email:s.vanshi@op.iitg.ac.in

---

## ðŸ“ž Support & Questions

- **Issues**: Open GitHub Issues for bugs and feature requests
- **Discussions**: Use GitHub Discussions for questions and ideas
- **Email**: vanshisethi1815@gmail.com

---

## ðŸ™ Acknowledgments

- **Google** for Gemini API and generative AI capabilities
- **Open-source community** for scikit-learn, pandas, and Streamlit
- **Financial data providers** (yfinance, Yahoo Finance)

---

**Last Updated**: February 5, 2026  
**Status**: âœ… Production Ready  
**Version**: 1.0.0
